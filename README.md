# ğŸ§  Falcon LegalBot â€” Fine-Tuning Falcon with LoRA for Indian Legal QA

This project implements a custom legal question-answering chatbot using the Falcon-RW-1B language model, fine-tuned with LoRA (Parameter-Efficient Fine-Tuning) on a dataset of legal instructions and responses.

## ğŸ“Œ Highlights

- âœ… Fine-tuned using [LoRA (PEFT)](https://huggingface.co/docs/peft) on top of `tiiuae/falcon-rw-1b`
- âœ… Used 4-bit quantization via `bitsandbytes` for low-memory training
- âœ… Trained on a custom dataset of 10,000 legal Q&A examples
- âœ… Achieved stable convergence in under 3 epochs (loss â‰ˆ 3.02)
- âœ… Optimized for low-cost training on Colab with `fp16`, batching & gradient accumulation

---

## ğŸš€ Quick Overview

### Dataset
Custom-built legal Q&A dataset in the following format:
```json
{
  "instruction": "Explain Section 498A of the IPC.",
  "output": "Section 498A deals with cruelty to a woman by her husband or his relatives..."
}
```

### Training Framework
- Hugging Face `transformers`
- `peft` with LoRA (8- or 16-rank adapters)
- PyTorch + `accelerate` for hardware efficiency

---

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

Youâ€™ll also need a CUDA-enabled GPU if not running on Colab.

---

## ğŸ—ƒï¸ Files Included

- `legal_bot_training.ipynb` â€” Full Colab notebook for training
- `legal_bot_dataset.json` â€” Custom dataset (10,000 entries)
- `requirements.txt` â€” All dependencies
- `README.md` â€” This file

---

## ğŸ¤– Future Work

- Integrate RAG for external legal document lookup
- Add Gradio UI for interactive legal chatbot
- Deploy model to Hugging Face Spaces or Streamlit

---
---
